# Introduction to Gradient Descent: Convergence in theory and practice

Welcome to the GitHub page that is complimentary to the bachelor' report '*Introduction to Gradient Descent: Convergence in theory and practice*'

## Source code and project structure

The figures in the report has been generated using the graphical library MatPlotLib. Open any scripts in the project's root directory, such as 

```python non_convex_L2_convergence_standard.py```

to run the non-convex regularization experiment.

No modules contain any scripts meant to be executed. All scripts in the project root are experiments which have been performed in this paper.

Implementations of optimizers are found in the `./algorithms/` module. Implementations of different models are found in the `./models/` module.

## Data sets
We have used the following two data sets primarily
 - Wine quality (https://www.kaggle.com/datasets/yasserh/wine-quality-dataset)
 - MNIST (https://www.kaggle.com/datasets/oddrationale/mnist-in-csv)

**Note**: In order to run the python experiment scripts, you will **NOT** have to download these datasets, they will be downloaded automatically when script using requiring it is executed. This is just for reference.

### Authors:
- Casper Dalgaard Nielsen, 202004453
- Mads Kongstad Laugesen, 202008953
- Andreas Malthe Henriksen, 202004107

### License
Copyright 2023 [MIT](LICENSE) Â© Caspar Dalgaard Nielsen, Mads Kongstad Laugesen, A. Malthe Henriksen 

